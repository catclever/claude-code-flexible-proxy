# æ™ºèƒ½å¤šæ¨¡å‹ä»£ç†æœåŠ¡å™¨ ğŸ”„

**è®© Claude Code ç­‰ Anthropic å®¢æˆ·ç«¯æ— ç¼ä½¿ç”¨ 60+ ç§ AI æ¨¡å‹** ğŸ¤

ä¸€ä¸ªå¼ºå¤§çš„ä»£ç†æœåŠ¡å™¨ï¼Œæ”¯æŒ OpenAIã€Google Geminiã€Anthropic ä¸‰å¤§å¹³å°çš„ 60+ ç§æ¨¡å‹ï¼Œå…·å¤‡å®æ—¶æ¨¡å‹åˆ‡æ¢ã€å¯¹è¯å†å²ç®¡ç†ã€è·¨å¹³å°é…ç½®ç­‰é«˜çº§åŠŸèƒ½ã€‚é€šè¿‡ LiteLLM å®ç°ç»Ÿä¸€çš„ API æ¥å£ã€‚ğŸŒ‰

![Anthropic API Proxy](pic.png)

## âœ¨ æ ¸å¿ƒç‰¹æ€§

- ğŸš€ **60+ æ¨¡å‹æ”¯æŒ**: OpenAI (30+) + Gemini (15+) + Anthropic (13+)
- ğŸ”„ **å®æ—¶æ¨¡å‹åˆ‡æ¢**: å¯¹è¯ä¸­æ— ç¼åˆ‡æ¢ä¸åŒæ¨¡å‹
- ğŸ›ï¸ **7ç§é¢„è®¾é…ç½®**: ä¸€é”®åˆ‡æ¢æœ€ä½³æ¨¡å‹ç»„åˆ
- ğŸ’¬ **å¯¹è¯å†å²ç®¡ç†**: è‡ªåŠ¨è®°å½•å’Œç®¡ç†å¯¹è¯ä¸Šä¸‹æ–‡
- ğŸŒ **è·¨å¹³å°é…ç½®**: å¤§å°æ¨¡å‹å¯æ¥è‡ªä¸åŒæä¾›å•†
- ğŸ–¥ï¸ **äº¤äº’å¼ç»ˆç«¯**: å¯è§†åŒ–é…ç½®å’Œç®¡ç†ç•Œé¢
- ğŸ“Š **æ™ºèƒ½æ¨¡å‹æ˜ å°„**: è‡ªåŠ¨è¯†åˆ«æ¨¡å‹æä¾›å•†å¹¶æ·»åŠ æ­£ç¡®å‰ç¼€

## Quick Start âš¡

### æ–¹å¼ä¸€ï¼šäº¤äº’å¼å¯åŠ¨ï¼ˆæ¨èï¼‰ ğŸ®

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/1rgs/claude-code-proxy.git
cd claude-code-proxy

# é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env
# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œå¡«å…¥ä½ çš„ API Keys

# å¯åŠ¨äº¤äº’å¼åº”ç”¨
uv run python interactive.py
```

äº¤äº’å¼åº”ç”¨æä¾›ï¼š
- ğŸ›ï¸ å¯è§†åŒ–é…ç½®ç•Œé¢
- ğŸ”„ å®æ—¶æ¨¡å‹åˆ‡æ¢
- ğŸ“Š æœåŠ¡å™¨çŠ¶æ€ç›‘æ§
- ğŸ’¬ å¯¹è¯å†å²ç®¡ç†
- ğŸ§ª é…ç½®æµ‹è¯•åŠŸèƒ½

### æ–¹å¼äºŒï¼šä¼ ç»ŸæœåŠ¡å™¨æ¨¡å¼ ğŸ–¥ï¸

#### Prerequisites

- OpenAI API key ğŸ”‘
- Google AI Studio (Gemini) API key (å¯é€‰) ğŸ”‘
- Anthropic API key (å¯é€‰) ğŸ”‘
- [uv](https://github.com/astral-sh/uv) installed

#### From source

1. **Clone this repository**:
   ```bash
   git clone https://github.com/1rgs/claude-code-proxy.git
   cd claude-code-proxy
   ```

2. **Install uv** (if you haven't already):
   ```bash
   curl -LsSf https://astral.sh/uv/install.sh | sh
   ```
   *(`uv` will handle dependencies based on `pyproject.toml` when you run the server)*

3. **Configure Environment Variables**:
   Copy the example environment file:
   ```bash
   cp .env.example .env
   ```
   Edit `.env` and fill in your API keys and model configurations:

   *   `ANTHROPIC_API_KEY`: (Optional) Needed only if proxying *to* Anthropic models.
   *   `OPENAI_API_KEY`: Your OpenAI API key (Required if using the default OpenAI preference or as fallback).
   *   `GEMINI_API_KEY`: Your Google AI Studio (Gemini) API key (Required if PREFERRED_PROVIDER=google).
   *   `PREFERRED_PROVIDER` (Optional): Set to `openai` (default), `google`, or `anthropic`. This determines the primary backend for mapping `haiku`/`sonnet`.
   *   `BIG_MODEL` (Optional): The model to map `sonnet` requests to. Defaults to `gpt-4.1` (if `PREFERRED_PROVIDER=openai`) or `gemini-2.5-pro-preview-03-25`. Ignored when `PREFERRED_PROVIDER=anthropic`.
   *   `SMALL_MODEL` (Optional): The model to map `haiku` requests to. Defaults to `gpt-4.1-mini` (if `PREFERRED_PROVIDER=openai`) or `gemini-2.0-flash`. Ignored when `PREFERRED_PROVIDER=anthropic`.

   **Mapping Logic:**
   - If `PREFERRED_PROVIDER=openai` (default), `haiku`/`sonnet` map to `SMALL_MODEL`/`BIG_MODEL` prefixed with `openai/`.
   - If `PREFERRED_PROVIDER=google`, `haiku`/`sonnet` map to `SMALL_MODEL`/`BIG_MODEL` prefixed with `gemini/` *if* those models are in the server's known `GEMINI_MODELS` list (otherwise falls back to OpenAI mapping).
   - If `PREFERRED_PROVIDER=anthropic`, `haiku`/`sonnet` requests are passed directly to Anthropic with the `anthropic/` prefix without remapping to different models.

4. **Run the server**:
   ```bash
   uv run uvicorn server:app --host 0.0.0.0 --port 8082 --reload
   ```
   *(`--reload` is optional, for development)*

#### Docker

If using docker, download the example environment file to `.env` and edit it as described above.
```bash
curl -O .env https://raw.githubusercontent.com/1rgs/claude-code-proxy/refs/heads/main/.env.example
```

Then, you can either start the container with [docker compose](https://docs.docker.com/compose/) (preferred):

```yml
services:
  proxy:
    image: ghcr.io/1rgs/claude-code-proxy:latest
    restart: unless-stopped
    env_file: .env
    ports:
      - 8082:8082
```

Or with a command:

```bash
docker run -d --env-file .env -p 8082:8082 ghcr.io/1rgs/claude-code-proxy:latest
```

### Using with Claude Code ğŸ®

1. **Install Claude Code** (if you haven't already):
   ```bash
   npm install -g @anthropic-ai/claude-code
   ```

2. **Connect to your proxy**:
   ```bash
   ANTHROPIC_BASE_URL=http://localhost:8082 claude
   ```

3. **That's it!** Your Claude Code client will now use the configured backend models (defaulting to Gemini) through the proxy. ğŸ¯

## ğŸ¯ æ™ºèƒ½æ¨¡å‹æ˜ å°„ä¸é…ç½®

### æ¨¡å‹æ˜ å°„é€»è¾‘

ä»£ç†æœåŠ¡å™¨è‡ªåŠ¨å°† Claude æ¨¡å‹æ˜ å°„åˆ°é…ç½®çš„åç«¯æ¨¡å‹ï¼š

| Claude æ¨¡å‹ | æ˜ å°„ç›®æ ‡ | è¯´æ˜ |
|------------|---------|------|
| `haiku` | `SMALL_MODEL` | å¿«é€Ÿå“åº”æ¨¡å‹ |
| `sonnet` | `BIG_MODEL` | é«˜æ€§èƒ½æ¨¡å‹ |

### ğŸš€ æ”¯æŒçš„ 60+ æ¨¡å‹

#### OpenAI æ¨¡å‹ (30+)
**O ç³»åˆ— (æ¨ç†ä¼˜åŒ–)**
- `o3-mini`, `o1`, `o1-mini`, `o1-pro`, `o1-preview`

**GPT-4o ç³»åˆ— (æœ€æ–°æ——èˆ°)**
- `gpt-4o`, `gpt-4o-2024-11-20`, `gpt-4o-2024-08-06`, `gpt-4o-mini`
- `gpt-4o-audio-preview`, `gpt-4o-realtime-preview`, `chatgpt-4o-latest`

**GPT-4 Turbo ç³»åˆ—**
- `gpt-4-turbo`, `gpt-4-turbo-2024-04-09`, `gpt-4-0125-preview`

**ç»å…¸ GPT ç³»åˆ—**
- `gpt-4`, `gpt-3.5-turbo`, `gpt-3.5-turbo-16k`

#### Google Gemini æ¨¡å‹ (15+)
**Gemini 2.0 ç³»åˆ— (æœ€æ–°)**
- `gemini-2.0-flash-exp`, `gemini-2.0-flash-thinking-exp`, `gemini-2.0-flash`

**Gemini 1.5 ç³»åˆ— (ä¸»åŠ›)**
- `gemini-1.5-pro`, `gemini-1.5-flash`, `gemini-1.5-flash-8b`
- `gemini-1.5-pro-002`, `gemini-1.5-flash-002`

**å®éªŒç‰ˆæœ¬**
- `gemini-exp-1114`, `gemini-exp-1121`, `gemini-2.5-pro`

#### Anthropic æ¨¡å‹ (13+)
**Claude 3.5 ç³»åˆ— (æœ€æ–°)**
- `claude-3-5-sonnet-20241022`, `claude-3-5-haiku-20241022`

**Claude 3 ç³»åˆ—**
- `claude-3-opus-20240229`, `claude-3-sonnet-20240229`, `claude-3-haiku-20240307`

**ç»å…¸ç‰ˆæœ¬**
- `claude-2.1`, `claude-2.0`, `claude-instant-1.2`

### ğŸ›ï¸ 7ç§é¢„è®¾é…ç½®

| é¢„è®¾åç§° | å¤§æ¨¡å‹ | å°æ¨¡å‹ | é€‚ç”¨åœºæ™¯ |
|---------|--------|--------|----------|
| `openai_premium` | GPT-4o | GPT-4o Mini | OpenAI é«˜ç«¯ä½“éªŒ |
| `gemini_balanced` | Gemini 1.5 Pro | Gemini 1.5 Flash | Google AI å¹³è¡¡ |
| `anthropic_pure` | Claude 3.5 Sonnet | Claude 3.5 Haiku | åŸç”Ÿ Claude ä½“éªŒ |
| `cross_premium` | GPT-4o | Gemini Flash | è·¨å¹³å°é«˜ç«¯ |
| `cross_balanced` | Gemini Pro | GPT-4o Mini | è·¨å¹³å°ç»æµ |
| `openai_reasoning` | O1 Preview | GPT-4o Mini | æ¨ç†ä¼˜åŒ– |
| `cross_speed` | Claude Haiku | Gemini Flash | é€Ÿåº¦ä¼˜å…ˆ |

## âš™ï¸ é…ç½®ç®¡ç†

### ç¯å¢ƒå˜é‡é…ç½®

åˆ›å»º `.env` æ–‡ä»¶å¹¶é…ç½®ä»¥ä¸‹å˜é‡ï¼š

```dotenv
# API Keys (è‡³å°‘éœ€è¦ä¸€ä¸ª)
OPENAI_API_KEY="your-openai-key"
GEMINI_API_KEY="your-google-key"
ANTHROPIC_API_KEY="your-anthropic-key"

# å¯é€‰ï¼šé»˜è®¤æ¨¡å‹é…ç½®
BIG_MODEL="gpt-4o"
SMALL_MODEL="gpt-4o-mini"

# å¯é€‰ï¼šä»£ç†æœåŠ¡å™¨è®¾ç½®
PROXY_SERVER_ENABLED="true"
```

### é…ç½®æ–¹å¼å¯¹æ¯”

| é…ç½®æ–¹å¼ | ä¼˜åŠ¿ | é€‚ç”¨åœºæ™¯ |
|---------|------|----------|
| **äº¤äº’å¼ç»ˆç«¯** | å¯è§†åŒ–ã€å®æ—¶åˆ‡æ¢ | å¼€å‘è°ƒè¯•ã€é…ç½®ç®¡ç† |
| **API æ¥å£** | ç¨‹åºåŒ–æ§åˆ¶ | è‡ªåŠ¨åŒ–ã€é›†æˆåº”ç”¨ |
| **ç¯å¢ƒå˜é‡** | ç®€å•ç¨³å®š | ç”Ÿäº§éƒ¨ç½²ã€å®¹å™¨åŒ– |
| **é¢„è®¾é…ç½®** | ä¸€é”®åˆ‡æ¢ | å¿«é€Ÿä½“éªŒã€åœºæ™¯åˆ‡æ¢ |

### é«˜çº§é…ç½®é€‰é¡¹

#### 1. è·¨å¹³å°æ¨¡å‹ç»„åˆ
```bash
# å¤§æ¨¡å‹ç”¨ Claudeï¼Œå°æ¨¡å‹ç”¨ Gemini
POST /switch_model
{
  "big_model": "claude-3-5-sonnet-20241022",
  "small_model": "gemini-1.5-flash"
}
```

#### 2. å¯¹è¯å†å²æŒä¹…åŒ–
```bash
# å¯ç”¨å¯¹è¯è®°å½•åˆ°æ–‡ä»¶
POST /conversation/enable
{
  "file_path": "/path/to/conversation.json"
}
```

#### 3. è°ƒè¯•å’Œç›‘æ§
```bash
# æŸ¥çœ‹è°ƒè¯•æ—¥å¿—
GET /debug_logs?log_type=model_mapping&limit=50

# æŸ¥çœ‹è¯·æ±‚ç»Ÿè®¡
GET /conversation/status
```

## ğŸ”„ å®æ—¶æ¨¡å‹åˆ‡æ¢ä¸å¯¹è¯ç®¡ç†

### æ¨¡å‹åˆ‡æ¢ API

#### æŸ¥çœ‹å½“å‰é…ç½®
```bash
GET /config
```

#### ä½¿ç”¨é¢„è®¾å¿«é€Ÿåˆ‡æ¢
```bash
POST /switch_model
Content-Type: application/json

{
  "preset": "cross_premium",
  "preserve_conversation": true
}
```

#### è‡ªå®šä¹‰è·¨å¹³å°é…ç½®
```bash
POST /switch_model
Content-Type: application/json

{
  "big_model": "claude-3-5-sonnet-20241022",
  "small_model": "gpt-4o-mini",
  "preserve_conversation": true
}
```

#### æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ¨¡å‹å’Œé¢„è®¾
```bash
GET /models
```

### å¯¹è¯å†å²ç®¡ç† API

#### æŸ¥çœ‹å¯¹è¯å†å²
```bash
GET /conversation?limit=10
```

è¿”å›ç¤ºä¾‹ï¼š
```json
{
  "messages": [
    {
      "timestamp": "2024-01-01T12:00:00",
      "role": "user",
      "content": "ä½ å¥½",
      "model_used": "openai/gpt-4o-mini"
    },
    {
      "timestamp": "2024-01-01T12:00:01",
      "role": "assistant",
      "content": "ä½ å¥½ï¼æˆ‘æ˜¯åŠ©æ‰‹...",
      "model_used": "openai/gpt-4o-mini"
    }
  ],
  "total_count": 2
}
```

#### æ¸…ç©ºå¯¹è¯å†å²
```bash
DELETE /conversation
```

#### ä½¿ç”¨å†å²å¯¹è¯ç»§ç»­èŠå¤©
```bash
POST /conversation/continue
Content-Type: application/json

{
  "model": "sonnet",
  "max_tokens": 200,
  "messages": [
    {"role": "user", "content": "æ€»ç»“ä¸€ä¸‹æˆ‘ä»¬åˆšæ‰çš„å¯¹è¯"}
  ]
}
```

## ğŸ¯ ä½¿ç”¨åœºæ™¯ä¸ç¤ºä¾‹

### åœºæ™¯1ï¼šåœ¨å¯¹è¯ä¸­åˆ‡æ¢æ¨¡å‹
```bash
# 1. ç”¨ Claude Haiku å¼€å§‹å¯¹è¯
curl -X POST http://localhost:8082/v1/messages \
  -H "Content-Type: application/json" \
  -d '{
    "model": "haiku",
    "max_tokens": 100,
    "messages": [{"role": "user", "content": "ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}]
  }'

# 2. åˆ‡æ¢åˆ°æ›´å¼ºå¤§çš„æ¨¡å‹
curl -X POST http://localhost:8082/switch_model \
  -H "Content-Type: application/json" \
  -d '{
    "preset": "cross_premium",
    "preserve_conversation": true
  }'

# 3. ç»§ç»­å¯¹è¯ï¼Œç°åœ¨ä½¿ç”¨æ–°æ¨¡å‹ä½†ä¿æŒå†å²
curl -X POST http://localhost:8082/v1/messages \
  -H "Content-Type: application/json" \
  -d '{
    "model": "sonnet",
    "max_tokens": 200,
    "messages": [{"role": "user", "content": "è¯¦ç»†è§£é‡Šä¸€ä¸‹ä½ çš„èƒ½åŠ›"}]
  }'
```

### åœºæ™¯2ï¼šä½¿ç”¨äº¤äº’å¼ç»ˆç«¯ç®¡ç†
```bash
# å¯åŠ¨äº¤äº’å¼åº”ç”¨
uv run python interactive.py

# åœ¨äº¤äº’ç•Œé¢ä¸­ï¼š
# 1. æŸ¥çœ‹å½“å‰çŠ¶æ€
# 2. é€‰æ‹©é¢„è®¾é…ç½®
# 3. æµ‹è¯•é…ç½®
# 4. æŸ¥çœ‹å¯¹è¯å†å²
# 5. ç®¡ç†è°ƒè¯•æ—¥å¿—
```

### åœºæ™¯3ï¼šClaude Code é›†æˆ
```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export ANTHROPIC_BASE_URL=http://localhost:8082

# å¯åŠ¨ Claude Code
claude

# ç°åœ¨ Claude Code ä¼šé€šè¿‡ä»£ç†ä½¿ç”¨é…ç½®çš„æ¨¡å‹
# å¯ä»¥åœ¨å¯¹è¯ä¸­å®æ—¶åˆ‡æ¢æ¨¡å‹è€Œä¸ä¸­æ–­ä¼šè¯
```

## ğŸ”§ å·¥ä½œåŸç†

è¿™ä¸ªä»£ç†æœåŠ¡å™¨çš„å·¥ä½œæµç¨‹ï¼š

1. **æ¥æ”¶è¯·æ±‚** - æ¥æ”¶ Anthropic API æ ¼å¼çš„è¯·æ±‚ ğŸ“¥
2. **æ™ºèƒ½è·¯ç”±** - æ ¹æ®é…ç½®å°†è¯·æ±‚è·¯ç”±åˆ°å¯¹åº”çš„æä¾›å•† ğŸ¯
3. **æ ¼å¼è½¬æ¢** - é€šè¿‡ LiteLLM è¿›è¡Œ API æ ¼å¼è½¬æ¢ ğŸ”„
4. **æ¨¡å‹æ˜ å°„** - è‡ªåŠ¨æ·»åŠ æ­£ç¡®çš„æ¨¡å‹å‰ç¼€ ğŸ“Š
5. **å‘é€è¯·æ±‚** - å‘ç›®æ ‡æä¾›å•†å‘é€è¯·æ±‚ ğŸ“¤
6. **å“åº”è½¬æ¢** - å°†å“åº”è½¬æ¢å› Anthropic æ ¼å¼ ğŸ”„
7. **å†å²è®°å½•** - è‡ªåŠ¨è®°å½•å¯¹è¯å†å² ğŸ’¾
8. **è¿”å›ç»“æœ** - è¿”å›æ ¼å¼åŒ–çš„å“åº”ç»™å®¢æˆ·ç«¯ âœ…

æ”¯æŒæµå¼å’Œéæµå¼å“åº”ï¼Œä¸æ‰€æœ‰ Claude å®¢æˆ·ç«¯å®Œå…¨å…¼å®¹ã€‚ğŸŒŠ

## ğŸŒŸ é«˜çº§ç‰¹æ€§

### æ™ºèƒ½æ¨¡å‹å‰ç¼€å¤„ç†
- è‡ªåŠ¨è¯†åˆ«æ¨¡å‹æ‰€å±æä¾›å•†
- æ™ºèƒ½æ·»åŠ  `openai/`ã€`gemini/`ã€`anthropic/` å‰ç¼€
- æ”¯æŒæ¨¡å‹åç§°éªŒè¯å’Œé”™è¯¯æç¤º

### å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†
- è‡ªåŠ¨è®°å½•æ‰€æœ‰å¯¹è¯å†å²
- æ”¯æŒè·¨æ¨¡å‹åˆ‡æ¢æ—¶ä¿æŒä¸Šä¸‹æ–‡
- å¯é…ç½®å†å²è®°å½•æ•°é‡é™åˆ¶
- æ”¯æŒå¯¹è¯å¯¼å‡ºå’Œå¯¼å…¥

### è°ƒè¯•å’Œç›‘æ§
- å®æ—¶è¯·æ±‚æ—¥å¿—è®°å½•
- æ¨¡å‹æ˜ å°„è¿‡ç¨‹å¯è§†åŒ–
- æ€§èƒ½ç»Ÿè®¡å’Œé”™è¯¯è¿½è¸ª
- æ”¯æŒæ—¥å¿—å¯¼å‡ºå’Œåˆ†æ

### å®‰å…¨å’Œç¨³å®šæ€§
- API Key å®‰å…¨å­˜å‚¨
- è¯·æ±‚é¢‘ç‡é™åˆ¶
- é”™è¯¯é‡è¯•æœºåˆ¶
- ä¼˜é›…çš„æœåŠ¡é™çº§

## ğŸ³ Docker éƒ¨ç½²

### ä½¿ç”¨ Docker Composeï¼ˆæ¨èï¼‰
```yaml
services:
  claude-proxy:
    image: ghcr.io/1rgs/claude-code-proxy:latest
    restart: unless-stopped
    env_file: .env
    ports:
      - "8082:8082"
    volumes:
      - ./conversations:/app/conversations
      - ./logs:/app/logs
```

### ç›´æ¥è¿è¡Œ Docker
```bash
docker run -d \
  --name claude-proxy \
  --env-file .env \
  -p 8082:8082 \
  -v $(pwd)/conversations:/app/conversations \
  ghcr.io/1rgs/claude-code-proxy:latest
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**Q: æ¨¡å‹åˆ‡æ¢åå“åº”æ ¼å¼ä¸æ­£ç¡®**
A: æ£€æŸ¥ç›®æ ‡æ¨¡å‹æ˜¯å¦æ”¯æŒå½“å‰è¯·æ±‚æ ¼å¼ï¼ŒæŸäº›æ¨¡å‹å¯èƒ½ä¸æ”¯æŒç‰¹å®šåŠŸèƒ½

**Q: API Key æ— æ•ˆé”™è¯¯**
A: ç¡®è®¤ `.env` æ–‡ä»¶ä¸­çš„ API Key æ­£ç¡®ï¼Œå¹¶ä¸”å¯¹åº”çš„æä¾›å•†æœåŠ¡å¯ç”¨

**Q: å¯¹è¯å†å²ä¸¢å¤±**
A: æ£€æŸ¥ `preserve_conversation` å‚æ•°æ˜¯å¦è®¾ç½®ä¸º `true`

**Q: äº¤äº’å¼ç»ˆç«¯æ— æ³•å¯åŠ¨**
A: ç¡®ä¿å®‰è£…äº†æ‰€æœ‰ä¾èµ–ï¼š`uv sync`

### è°ƒè¯•æ¨¡å¼
```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export LOG_LEVEL=DEBUG
uv run python interactive.py

# æŸ¥çœ‹å®æ—¶æ—¥å¿—
curl http://localhost:8082/debug_logs?log_type=all&limit=100
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿å„ç§å½¢å¼çš„è´¡çŒ®ï¼ğŸ

### å¦‚ä½•è´¡çŒ®
1. Fork è¿™ä¸ªä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ï¼š`git checkout -b feature/amazing-feature`
3. æäº¤æ›´æ”¹ï¼š`git commit -m 'Add amazing feature'`
4. æ¨é€åˆ†æ”¯ï¼š`git push origin feature/amazing-feature`
5. åˆ›å»º Pull Request

### å¼€å‘ç¯å¢ƒè®¾ç½®
```bash
# å…‹éš†ä»“åº“
git clone https://github.com/1rgs/claude-code-proxy.git
cd claude-code-proxy

# å®‰è£…ä¾èµ–
uv sync

# è¿è¡Œæµ‹è¯•
uv run pytest

# å¯åŠ¨å¼€å‘æœåŠ¡å™¨
uv run uvicorn server:app --reload --host 0.0.0.0 --port 8082
```

### è´¡çŒ®ç±»å‹
- ğŸ› Bug ä¿®å¤
- âœ¨ æ–°åŠŸèƒ½å¼€å‘
- ğŸ“š æ–‡æ¡£æ”¹è¿›
- ğŸ¨ UI/UX ä¼˜åŒ–
- âš¡ æ€§èƒ½ä¼˜åŒ–
- ğŸ§ª æµ‹è¯•è¦†ç›–

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## ğŸ™ è‡´è°¢

- [LiteLLM](https://github.com/BerriAI/litellm) - ç»Ÿä¸€çš„ LLM API æ¥å£
- [FastAPI](https://fastapi.tiangolo.com/) - ç°ä»£åŒ–çš„ Web æ¡†æ¶
- [Rich](https://github.com/Textualize/rich) - ç¾è§‚çš„ç»ˆç«¯ç•Œé¢
- [Anthropic](https://www.anthropic.com/) - Claude API
- [OpenAI](https://openai.com/) - GPT API
- [Google](https://ai.google.dev/) - Gemini API
